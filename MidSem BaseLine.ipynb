{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "5AISi3R42ajn"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import math\n",
        "import string\n",
        "import urllib.request  \n",
        "import bs4 as BeautifulSoup\n",
        "import nltk\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize \n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urlopen\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from urllib.parse import urlparse\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from heapq import nlargest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbJb4R2r0702",
        "outputId": "601d4e9e-7b6b-427c-88c0-6be23474fe93"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "XX8beyKdCl7A"
      },
      "outputs": [],
      "source": [
        "def preprocess_query(query):\n",
        "    query = query.lower()\n",
        "    query = query.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    query = \" \".join(word for word in query.split() if word not in stop_words)\n",
        "    return query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "5dXVWEYyHLFJ"
      },
      "outputs": [],
      "source": [
        "def url():\n",
        "    query = input('Enter the query (\\\\\\'): ')\n",
        "    query = preprocess_query(query)\n",
        "    url = \"https://www.google.com/search?q=\" + query\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    search_results = soup.find_all('a')\n",
        "    urls = {urlparse(link.get('href')[7:].split('&')[0]).hostname: link.get('href')[7:].split('&')[0] \n",
        "            for link in search_results \n",
        "            if link.get('href').startswith('/url?q=')}\n",
        "    return urls  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "4Ia3E_894uG6"
      },
      "outputs": [],
      "source": [
        "def text_extract(links):\n",
        "  text1={}\n",
        "  for domain,url in links.items():\n",
        "   response = requests.get(url)\n",
        "   text =response.text \n",
        "   text1[domain]=text\n",
        "   article_content={}\n",
        "  for domain1,files in text1.items():\n",
        "# Parsing the URL content \n",
        "    article_parsed = BeautifulSoup(text1[domain1],'html.parser')\n",
        "  # Returning <p> tags\n",
        "    paragraphs = article_parsed.find_all('p')\n",
        "  # To get the content within all poaragrphs loop through it\n",
        "    article = ''\n",
        "    for p in paragraphs:  \n",
        "          article += p.text\n",
        "    article_content[domain1]=article\n",
        "  return article_content  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "hH3apWlXLDXr"
      },
      "outputs": [],
      "source": [
        "def filter_2000(text):\n",
        "  dict = {}\n",
        "\n",
        "  # Define the minimum number of characters an HTML file must have to be kept\n",
        "  min_chars = 2000\n",
        "\n",
        "  # Loop through each HTML file in the original dictionary\n",
        "  for key, value in text.items():\n",
        "      num_chars = len(value)\n",
        "      \n",
        "      # Check if the number of characters is greater than the minimum threshold\n",
        "      if num_chars >= min_chars:\n",
        "          # Add the HTML file to the filtered dictionary\n",
        "          dict[key] = value\n",
        "  return dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "t4bkHmbwtJ9H"
      },
      "outputs": [],
      "source": [
        "def preprocess_dict(dictionary):\n",
        "    processed_dict = {}\n",
        "    punctuation1='!\"#$%&\\'()*+-/<=>?@[\\\\]^_`{|}~'\n",
        "    for key, value in dictionary.items():\n",
        "        # convert to lowercase\n",
        "        value = value.lower()\n",
        "        # remove punctuation\n",
        "        value = \"\".join([char for char in value if char not in punctuation1 or  '.'])\n",
        "        # remove whitespace\n",
        "        value = \" \".join(value.split())\n",
        "        # tokenize\n",
        "        tokens = nltk.word_tokenize(value)\n",
        "        # remove stop words\n",
        "        tokens = [token for token in tokens if token not in stop_words]\n",
        "        #stemmer = PorterStemmer()\n",
        "        #tokens = [stemmer.stem(token) for token in tokens]\n",
        "        # join tokens back into a string\n",
        "        processed_dict[key] = \" \".join(tokens)\n",
        "    return processed_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "64QAwolBgdSn"
      },
      "outputs": [],
      "source": [
        "def frequency(text):   \n",
        " frequencies = {}\n",
        " for key,val in text.items():\n",
        "    word_frequencies = {}\n",
        "    for word in val.split():    \n",
        "      if word.lower() not in stop_words:\n",
        "              if word not in word_frequencies.keys():\n",
        "                  word_frequencies[word] = 1\n",
        "              else:\n",
        "                  word_frequencies[word] += 1\n",
        "    frequencies[key]=word_frequencies\n",
        " for key,val in frequencies.items():\n",
        "   max_freq=max(frequencies[key].values())\n",
        "   for k,v in val.items():\n",
        "       val[k]=val[k]/max_freq \n",
        " return frequencies                      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "Ri2h070l6BLY"
      },
      "outputs": [],
      "source": [
        "def text_summary(text,a):\n",
        "  sent_token={}\n",
        "  for key,val in text.items():\n",
        "   sent_token[key] = sent_tokenize(val)\n",
        "  #sentence_scores = {}\n",
        "  scores={}\n",
        "  for k, sent in sent_token.items():\n",
        "    sentence_scores = {}\n",
        "    # Loop through each sentence in the current key's list of sentences\n",
        "    for val in sent:\n",
        "      \n",
        "      # Split the sentence into individual words\n",
        "      words = val.split(\" \")\n",
        "      \n",
        "      # Initialize a score for the current sentence\n",
        "      score = 0\n",
        "      \n",
        "      # Loop through each word in the current sentence\n",
        "      for word in words:\n",
        "          \n",
        "          # Check if the word is in the frequency dictionary for the current key\n",
        "          if word.lower() in a.get(k, {}):\n",
        "              \n",
        "              # If the word is in the frequency dictionary, add its frequency to the score\n",
        "              score += a[k][word.lower()]\n",
        "      \n",
        "      # Add the score for the current sentence to the sentence_scores dictionary\n",
        "      sentence_scores[val] = score\n",
        "      \n",
        "    scores[k]=sentence_scores \n",
        "  summaries={}\n",
        "  for k,val in sent_token.items(): \n",
        "    select_length = int(len(val)*0.3)\n",
        "    #print(select_length)\n",
        "    \n",
        "    summary = nlargest(select_length, scores[k], key = scores[k].get)\n",
        "    final_summary = [word for word in summary]\n",
        "    summary = ' '.join(final_summary)\n",
        "    summaries[k]=summary\n",
        "  return summaries    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "1oWkxeTKB852"
      },
      "outputs": [],
      "source": [
        "def execution_flow():\n",
        "  link=url()\n",
        "  text=text_extract(link)\n",
        "  filter=filter_2000(text)\n",
        "  dic=preprocess_dict(filter)\n",
        "  fre=frequency(dic)\n",
        "  summary=text_summary(dic,fre)\n",
        "  return summary\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9nGVEkIGx9z",
        "outputId": "307304d1-8066-44c6-c370-fc7dac5893f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the query (\\'): web application\n",
            "https://www.www.techtarget.com:1408\n",
            "https://www.en.wikipedia.org:1641\n",
            "https://www.www.stackpath.com:999\n",
            "https://www.cs.lmu.edu:1826\n",
            "https://www.blog.hubspot.com:4088\n",
            "https://www.www.britannica.com:1793\n"
          ]
        }
      ],
      "source": [
        "ans=execution_flow()\n",
        "len(ans)\n",
        "for link,text in ans.items():\n",
        "    url1='https://www.'\n",
        "    print(url1 + str(link) + ':'+str(text))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
