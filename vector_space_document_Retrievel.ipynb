{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfBVDvGLHtCy"
      },
      "source": [
        "#Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Jj0IiVmEHhyv"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "from math import *\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sx8vN9olrs15",
        "outputId": "0eae49ca-c78b-4611-8343-40a07b28a259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fever-baselines'...\n",
            "remote: Enumerating objects: 3092, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 3092 (delta 13), reused 1 (delta 0), pack-reused 3062\u001b[K\n",
            "Receiving objects: 100% (3092/3092), 1.04 MiB | 22.56 MiB/s, done.\n",
            "Resolving deltas: 100% (2184/2184), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/sheffieldnlp/fever-baselines.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2AhiZDSz7bG",
        "outputId": "7e13951b-6223-4437-e775-475996b697dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config\t    fever-baselines  README.md\t       scripts\n",
            "Dockerfile  LICENSE\t     requirements.txt  src\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUFqTNA303dP",
        "outputId": "f8f65f3d-07fb-4353-d924-8c4dfad191ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/fever-baselines\n"
          ]
        }
      ],
      "source": [
        "%cd /content/fever-baselines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkerSJIv1DS8",
        "outputId": "0d53cb00-86f8-4c82-ce77-3fbaf064d4b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config\t    fever-baselines  README.md\t       scripts\n",
            "Dockerfile  LICENSE\t     requirements.txt  src\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "02QrhbnNrszI"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/fever-baselines')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meRlVYH-IDAy"
      },
      "source": [
        "#Importing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PU7sXsfOHwza",
        "outputId": "b9bc665a-6acf-4824-ffa9-5edbcb8cb835"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "A9d3pIEjJ9wT"
      },
      "outputs": [],
      "source": [
        "PATH = '/content/gdrive/My Drive/project_work/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Vr8fqp17IIxs"
      },
      "outputs": [],
      "source": [
        "def similar(a, b, norm=False): #defining cosine similarity\n",
        "    assert len(a) == len(b)\n",
        "    given_list = [0] * len(b)\n",
        "    if a == given_list or b == given_list:\n",
        "        return float(1) if a == b else float(0)\n",
        "    \n",
        "    for i in range(len(a)):  \n",
        "       x = np.array([a[i] * b[i], a[i] * a[i], b[i] * b[i]])\n",
        "    y = sum(x[:, 0]) / (np.sqrt(sum(x[:, 1])) * np.sqrt(sum(x[:, 2])))\n",
        "    \n",
        "    return 0.5 * y + 0.5 if norm else y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImiF6zNpJYDc"
      },
      "source": [
        "#Extraction from wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Tseck270JcK8"
      },
      "outputs": [],
      "source": [
        "def wikip(wiki_dir): \n",
        "    dicted = dict()\n",
        "    for i in range(1,110):\n",
        "        jnum=\"{:03d}\".format(i)\n",
        "        fname=wiki_dir+\"wiki-\"+jnum+\".jsonl\"\n",
        "        f=open(fname)\n",
        "        sen=f.readline()\n",
        "        while sen:\n",
        "          dt=json.loads(sen.rstrip(\"\\n\"))\n",
        "          doc_id=dt[\"id\"]\n",
        "          tt = dt[\"text\"]\n",
        "          if tt != \"\":\n",
        "              dicted[doc_id]=tt\n",
        "          sen=f.readline()\n",
        "    np.save(PATH + \"my_task2.npy\",dicted)\n",
        "    print(\"It is completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzurGHe1Mlwf"
      },
      "source": [
        "#Create Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EJD3W3GWLdWO"
      },
      "outputs": [],
      "source": [
        "def create_dictionary(dicted): #Create dictionary\n",
        "    dt = np.load(dicted, allow_pickle=True).item()\n",
        "    dictionary = {}\n",
        "    number_ducument = len(dt)\n",
        "    for d in dt.items():\n",
        "        text = list(set(d[1].split(' ')))[1:]\n",
        "        for i in text:\n",
        "            i = re.sub(\"[,.。:_=+*&^%$#@!?()<>/`';|]\", \"\", i)\n",
        "            if i.isdigit():\n",
        "                continue\n",
        "            if not i in dictionary:\n",
        "                dictionary[i] = [d[0]]\n",
        "            else:\n",
        "                dictionary[i].append(d[0])\n",
        "    np.save(PATH + \"my_task2.npy\",dictionary)\n",
        "    print(number_ducument)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CpSjjKCQIil"
      },
      "source": [
        "#TF-IDF terms in the claim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "zoPiABxqOcVO"
      },
      "outputs": [],
      "source": [
        "def Subtask2_cossim(claim_id, numberofducuments):\n",
        "    data = np.load(PATH + 'my_task2.npy', allow_pickle=True).item()\n",
        "    dictory = np.load(PATH + 'my_task2.npy', allow_pickle=True).item()\n",
        "    td = np.load('/content/fever-baselines/config/test.json')\n",
        "\n",
        "    clm = None\n",
        "    for d in td:\n",
        "        if d['id'] == claim_id:\n",
        "            d['claim'] = re.sub(\"[,.。:_=+*&^%$#@!?()<>/`';|]\", \"\", d['claim'])\n",
        "            clm = d['claim'].split(' ')\n",
        "            break\n",
        "    print(d['id'])\n",
        "    print(d['claim'])\n",
        "\n",
        "    claim_tfidf = []\n",
        "    ky = []\n",
        "    for i in clm:\n",
        "        tf = clm.count(i) / len(clm)\n",
        "        idf = log((numberofducuments / (1 + (len(dictory[i]) if i in dictory else 0))))\n",
        "        ky.append((i, tf * idf, idf, tf))\n",
        "    ky.sort(key=lambda x: x[1], reverse=True)\n",
        "    ky = ky[:5]\n",
        "    word = [k[0] for k in ky]\n",
        "    vec1 = [k[1] for k in ky]\n",
        "    print(word)\n",
        "    print(vec1)\n",
        "    \n",
        "    doc_tf_idf = []\n",
        "    for d in data.items():\n",
        "        tt = d[1].split(' ')\n",
        "        vec2 = []\n",
        "        for k in ky:\n",
        "            tf = tt.count(k[0]) / len(tt)\n",
        "            idf = k[2]\n",
        "            vec2.append(tf * idf)\n",
        "        sm= similar(vec1, vec2)\n",
        "        doc_tf_idf.append([d[0], sm])\n",
        "    doc_tf_idf.sort(key=lambda x: x[1], reverse=True)\n",
        "    return doc_tf_idf[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Calling all funcations"
      ],
      "metadata": {
        "id": "UMRmmzq2AiDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wikip('/content/gdrive/MyDrive/project_work/data/wiki-pages/')"
      ],
      "metadata": {
        "id": "RGONtRguCnu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_dictionary(PATH + 'my_task2.npy')"
      ],
      "metadata": {
        "id": "J4qEbWOlAkeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOdjIrQ51pUZ"
      },
      "outputs": [],
      "source": [
        "index_list = [75397,150448,214861]\n",
        "length_of_ducuments = 5396106"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for index in index_list:\n",
        "    print(Subtask2_cossim(index, length_of_ducuments))"
      ],
      "metadata": {
        "id": "E2LCQq75C1Vg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}