# -*- coding: utf-8 -*-
"""Untitled28.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ytpJSHtbm1jyiohI0ggmcOML87g1W0MH
"""
#pip install nltk
import nltk
import math
import string
import urllib.request  
import bs4 as BeautifulSoup
import nltk
from string import punctuation
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize 
import requests
from bs4 import BeautifulSoup
from urllib.request import urlopen
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer
from urllib.parse import urlparse
from nltk.corpus import stopwords
import string
from heapq import nlargest
from gensim import corpora, models, similarities
from rank_bm25 import BM25Okapi
from evaluation import score
import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config

nltk.download('punkt')
nltk.download("stopwords")
stop_words = stopwords.words('english')

def preprocess_query(query):
    query = query.lower()
    query = query.translate(str.maketrans("", "", string.punctuation))
    stop_words = set(stopwords.words('english'))
    query = " ".join(word for word in query.split() if word not in stop_words)
    return query

def url(query):
    
    url = "https://www.googleapis.com/customsearch/v1"
    params = {
        # "key": "AIzaSyBv-yLsjwZNxP4iiLGJhgeMsMZwSULumFc",
        # "cx": "62626d6ed7e564621",
        "key": "AIzaSyAmwEicvjrOCn_dASPGvYC8xH4JlO87n88",
        "cx": "97de6257e52284b77",
        "q": query
    }
    response = requests.get(url, params=params)
    data = response.json()
    urls = {}
    # for item in data["items"]:
    for item in data.get("items", []):
        urls[urlparse(item["link"]).hostname] = item["link"]
    return urls

def text_extract(links):
    article_content = {}
    for domain, url in links.items():
        response = requests.get(url)
        text = response.text
        article_parsed = BeautifulSoup(text, 'html.parser')
        paragraphs = article_parsed.find_all('p')
        article = ''
        for p in paragraphs:
            article += p.text
        article_content[domain] = article
    return article_content


def filter_2000(text):
  dict = {}

  # Define the minimum number of characters an HTML file must have to be kept
  min_chars = 2000

  # Loop through each HTML file in the original dictionary
  for key, value in text.items():
      num_chars = len(value)
      
      # Check if the number of characters is greater than the minimum threshold
      if num_chars >= min_chars:
          # Add the HTML file to the filtered dictionary
          dict[key] = value
  return dict

def preprocess_dict(dictionary):
    processed_dict = {}
    punctuation1='!"#$%&\'()*+-/<=>?@[\\]^_`{|}~'
    for key, value in dictionary.items():
        # convert to lowercase
        value = value.lower()
        # remove punctuation
        value = "".join([char for char in value if char not in punctuation1 or  '.'])
        # remove whitespace
        value = " ".join(value.split())
        # tokenize
        tokens = nltk.word_tokenize(value)
        # remove stop words
        tokens = [token for token in tokens if token not in stop_words]
        #stemmer = PorterStemmer()
        #tokens = [stemmer.stem(token) for token in tokens]
        # join tokens back into a string
        processed_dict[key] = " ".join(tokens)
    return processed_dict

def frequency(text):   
 frequencies = {}
 for key,val in text.items():
    word_frequencies = {}
    for word in val.split():    
      if word.lower() not in stop_words:
              if word not in word_frequencies.keys():
                  word_frequencies[word] = 1
              else:
                  word_frequencies[word] += 1
    frequencies[key]=word_frequencies
 for key,val in frequencies.items():
   max_freq=max(frequencies[key].values())
   for k,v in val.items():
       val[k]=val[k]/max_freq 
 return frequencies

def text_summary(text,a):
  sent_token={}
  for key,val in text.items():
   sent_token[key] = sent_tokenize(val)
  #sentence_scores = {}
  scores={}
  for k, sent in sent_token.items():
    sentence_scores = {}
    # Loop through each sentence in the current key's list of sentences
    for val in sent:
      
      # Split the sentence into individual words
      words = val.split(" ")
      
      # Initialize a score for the current sentence
      score = 0
      
      # Loop through each word in the current sentence
      for word in words:
          
          # Check if the word is in the frequency dictionary for the current key
          if word.lower() in a.get(k, {}):
              
              # If the word is in the frequency dictionary, add its frequency to the score
              score += a[k][word.lower()]
      
      # Add the score for the current sentence to the sentence_scores dictionary
      sentence_scores[val] = score
      
    scores[k]=sentence_scores 
  summaries={}
  for k,val in sent_token.items(): 
    select_length = int(len(val)*0.3)
    #print(select_length)
    
    summary = nlargest(select_length, scores[k], key = scores[k].get)
    final_summary = [word for word in summary]
    summary = ' '.join(final_summary)
    summaries[k]=summary
  return summaries

def Rank(docs,query):
  bm25={}
  for links,content in docs.items():
    preprocessed_docs=[content.lower().split()]
    dictionary = corpora.Dictionary(preprocessed_docs)
    corpus = [dictionary.doc2bow(doc) for doc in preprocessed_docs]
    bm25[links] = BM25Okapi(corpus)
  # Tokenize the documents into a list of lists of words
  tokenized_docs = [doc.split() for doc in docs.values()]
  # Create a BM25 object
  bm25 = BM25Okapi(tokenized_docs)
  # Tokenize the query
  tokenized_query = query.split()
  # Get the document scores for the query
  scores = bm25.get_scores(tokenized_query)
  # Sort the documents by score in descending order
  #ranked_docs = sorted(summaries.keys(), key=lambda i: -scores[i-1])
  ranked_docs = sorted(docs.keys(), key=lambda k: -scores[list(docs.keys()).index(k)])
  #print(ranked_docs)
  ans={}
  # Print the top 3 documents
  for i in range(len(ranked_docs)):
      doc_id = ranked_docs[i]
      ans[doc_id]=docs[doc_id] 
  return ans

def summary(text):
      model = T5ForConditionalGeneration.from_pretrained('t5-small')
      tokenizer = T5Tokenizer.from_pretrained('t5-small')
      device = torch.device('cpu')
      tokenized_text={}
      for url,t in text.items():
          tokenized_text[url] = tokenizer.encode(t, return_tensors='pt', max_length=512).to(device)
      summary_dict={}
      for url,t in text.items():
        summary_ids = model.generate(tokenized_text[url], min_length=300, max_length=500)
        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True) 
        summary_dict[url]=summary 
      return summary_dict  
    

def execution_flow():
  q=input('Enter the query (\\\'): ')
  link=url(q)
  ref_text=text_extract(link)
  filter=filter_2000(ref_text)
  dic=preprocess_dict(filter)
  #print(dic)
  # fre=frequency(dic)
  # summary=text_summary(dic,fre)
  # #return summary
  # #fin_summary=summary(ref_text)
  # result=Rank(summary,q)
  # ans=score(ref_text,result)
  return filter

# ans=execution_flow()
# print(ans)
# for link in ans.keys():
#       url1='https://'
#       #print(link+ ':'+text)
#       print('*********************')
#       print(link+':'+ans[link][0])
#       print(': F_score of {link} is'+str(float(ans[link][1])))
#       print('\n')
      #print(ans)